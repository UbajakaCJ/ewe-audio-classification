{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import librosa\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Paths to the audio files (adjust based on your directory)\n",
    "audio_folder = 'C:/Users/Laptop/Documents/AI/zindi/ewe_1/files/audio_files/'\n",
    "train_csv_path = 'Train.csv'\n",
    "test_csv_path = 'Test_1.csv'\n",
    "submission_csv_path = 'SampleSubmission_1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files\n",
    "train_data = pd.read_csv(train_csv_path)\n",
    "test_data = pd.read_csv(test_csv_path)\n",
    "sample_submission = pd.read_csv(submission_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5334, 2946)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.17, random_state=58)\n",
    "\n",
    "for train_index, val_index in sss.split(train_data.drop('class', axis=1), train_data['class']):\n",
    "    train_df, val_df = train_data.iloc[train_index], train_data.iloc[val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df.reset_index()\n",
    "# train_df.pop('index')\n",
    "# # train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_df = val_df.reset_index()\n",
    "# val_df.pop('index')\n",
    "# # val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode labels (if necessary) \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_labels = le.fit_transform(train_df['class'])\n",
    "train_df['labels'] = train_labels\n",
    "\n",
    "le_val = LabelEncoder()\n",
    "val_labels = le_val.fit_transform(val_df['class'])\n",
    "val_df['labels'] = val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "# Bandpass filter (removes irrelevant frequency bands)\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def extract_mfcc(file_path, n_mfcc=40, max_len=100):\n",
    "    \"\"\"Extract MFCC features from an audio file.\"\"\"\n",
    "    audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "\n",
    "# Extract features (time-domain and frequency-domain)\n",
    "def extract_features(file_path, n_mels=40, max_length=100):  # audio_file => file_path\n",
    "    # Load the audio file\n",
    "    audio, sr = librosa.load(file_path)\n",
    "    \n",
    "    # Apply bandpass filter (e.g., remove frequencies outside 20-4000 Hz)\n",
    "    filtered_audio = bandpass_filter(audio, lowcut=20, highcut=4000, fs=sr)\n",
    "    \n",
    "    # Time-domain features\n",
    "    rms = librosa.feature.rms(y=filtered_audio)  # Root Mean Square energy\n",
    "    zcr = librosa.feature.zero_crossing_rate(filtered_audio)  # Zero-crossing rate\n",
    "    \n",
    "    # Time-frequency representation: Mel-spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=filtered_audio, sr=sr, n_mels=n_mels)\n",
    "    \n",
    "    # Convert to decibels for better representation\n",
    "    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # mfcc\n",
    "    mfcc = librosa.feature.mfcc(y=filtered_audio, sr=sr, n_mfcc=n_mels)\n",
    "   \n",
    "    # Ensure all features have the same length (pad/truncate)\n",
    "    def pad_or_truncate(feature, target_length=max_length):\n",
    "        if feature.shape[1] < target_length:\n",
    "            padding = target_length - feature.shape[1]\n",
    "            feature = np.pad(feature, ((0, 0), (0, padding)), 'constant')\n",
    "        else:\n",
    "            feature = feature[:, :target_length]\n",
    "        return feature\n",
    "\n",
    " \n",
    "    mel_spectrogram_db = pad_or_truncate(mel_spectrogram_db)\n",
    "    mfcc = pad_or_truncate(mfcc)\n",
    "\n",
    "    features =  (mel_spectrogram_db + mfcc) / 2\n",
    "\n",
    "    return features\n",
    "\n",
    "# Preprocess multiple audio files\n",
    "def preprocess_audio_files(audio_file_list, sample_rate=100, n_mels=40, max_length=100):\n",
    "    feature_tensors = []\n",
    "    for audio_file in audio_file_list:\n",
    "        features = extract_features(audio_file, sample_rate, n_mels, max_length)\n",
    "        feature_tensors.append(features)\n",
    "    \n",
    "    # Stack the tensors along a new batch dimension\n",
    "    batched_features = torch.stack(feature_tensors)\n",
    "\n",
    "    return batched_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract MFCCs\n",
    "def extract_mfcc(file_path, n_mfcc=40, max_len=100):\n",
    "    \"\"\"Extract MFCC features from an audio file.\"\"\"\n",
    "    audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "    \n",
    "    # Pad or truncate the mfccs to the same length\n",
    "    if mfccs.shape[1] < max_len:\n",
    "        pad_width = max_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfccs = mfccs[:, :max_len]\n",
    "    \n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset class for loading data\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data, audio_folder, is_train=True, transform=None):\n",
    "        self.data = data\n",
    "        self.audio_folder = audio_folder\n",
    "        self.is_train = is_train\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.data.loc[idx, 'audio_filepath']\n",
    "        file_path = self.audio_folder + file_name\n",
    "        if self.is_train:\n",
    "            label = self.data.loc[idx, 'labels']\n",
    "        else:\n",
    "            label = -1  # No label for test set\n",
    "\n",
    "        # Extract features\n",
    "        features = extract_features(file_path)\n",
    "        # features = extract_mfcc(file_path)\n",
    "\n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "\n",
    "        features = torch.tensor(features, dtype=torch.float32)\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN for classification\n",
    "class SpeechClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SpeechClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(32 * 10 * 25, 128)  # Adjust based on MFCC feature size\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 10 * 25)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and test datasets\n",
    "train_dataset = AudioDataset(train_df, audio_folder, is_train=True)\n",
    "val_dataset = AudioDataset(val_df, audio_folder, is_train=False)\n",
    "# test_dataset = AudioDataset(test_data, audio_folder, is_train=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "num_classes = len(train_df['labels'].unique())  # Assuming labels are integers\n",
    "model = SpeechClassifier(num_classes=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.6916\n",
      "Epoch [2/10], Loss: 0.1074\n",
      "Epoch [3/10], Loss: 0.0401\n",
      "Epoch [4/10], Loss: 0.0241\n",
      "Epoch [5/10], Loss: 0.0278\n",
      "Epoch [6/10], Loss: 0.0235\n",
      "Epoch [7/10], Loss: 0.0073\n",
      "Epoch [8/10], Loss: 0.0089\n",
      "Epoch [9/10], Loss: 0.0111\n",
      "Epoch [10/10], Loss: 0.0132\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for mfccs, labels in train_loader:\n",
    "        mfccs = mfccs.float()\n",
    "        labels = labels.long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # print(f'mfccs.shape: {mfccs.shape}')\n",
    "        outputs = model(mfccs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Predict on the val set\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for mfccs, _ in val_loader:\n",
    "        outputs = model(mfccs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9977949283351709"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.DataFrame(data={'predictions':predictions, 'labels': val_df['labels']})\n",
    "new_df['accuracy'] = new_df['predictions'] == new_df['labels']\n",
    "new_df['accuracy'] = new_df['accuracy'].apply(lambda x: int(x))\n",
    "new_df['accuracy'].sum()/len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9977949283351709\n",
      "Mean Squared Error: 0.037486218302094816\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy Score: {accuracy_score(new_df.predictions, new_df.labels)}')\n",
    "print(f'Mean Squared Error: {mean_squared_error(new_df.predictions, new_df.labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictions</th>\n",
       "      <th>labels</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     predictions  labels  accuracy\n",
       "153            2       7         0\n",
       "793            3       6         0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[new_df.accuracy == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(907, 4427)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_df), len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the prediction and the value labels\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "df = pd.DataFrame(new_df.labels)\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "encoded = lb.fit_transform(df['labels'])\n",
    "\n",
    "preds_df = pd.DataFrame(new_df.predictions)\n",
    "preds_encoded = lb.fit_transform(preds_df['predictions'])\n",
    "\n",
    "# cross check two data frames for accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9987168386979945"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc = roc_auc_score(encoded, preds_encoded )\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'y_pred' and 'y_true' are your predicted and true labels\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    # roc_auc = roc_auc_score(y_true, y_pred, average='macro', multi_class='ovo')\n",
    "\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-score:\", f1)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9977678571428572\n",
      "Recall: 0.9977477477477478\n",
      "F1-score: 0.9977477020473591\n",
      "Confusion Matrix:\n",
      " [[113   0   0   0   0   0   0   0]\n",
      " [  0 129   0   0   0   0   0   0]\n",
      " [  0   0 111   0   0   0   0   0]\n",
      " [  0   0   0 111   0   0   0   0]\n",
      " [  0   0   0   0 110   0   0   0]\n",
      " [  0   0   0   0   0 111   0   0]\n",
      " [  0   0   0   1   0   0 110   0]\n",
      " [  0   0   1   0   0   0   0 110]]\n",
      "Metrics: None\n"
     ]
    }
   ],
   "source": [
    "print(f'Metrics: {calculate_metrics(new_df.labels, new_df.predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For overfitting, evaluate on the train data \n",
    "\n",
    "# Predict on the train set\n",
    "model.eval()\n",
    "preds_train = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for mfccs, _ in train_loader:\n",
    "        outputs = model(mfccs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        preds_train.extend(predicted.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1348543031398238"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(preds_train, train_df['labels'])\n",
    "# 0.12536706573300202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.13447782231669847\n",
      "Recall: 0.13449287711118654\n",
      "F1-score: 0.13448480095958637\n",
      "Confusion Matrix:\n",
      " [[71 73 61 65 68 68 80 66]\n",
      " [72 97 84 78 68 72 86 73]\n",
      " [80 81 70 62 58 70 60 61]\n",
      " [56 81 64 70 72 65 53 75]\n",
      " [60 83 65 70 76 65 66 56]\n",
      " [75 65 63 70 67 70 76 58]\n",
      " [65 72 65 70 66 66 66 74]\n",
      " [73 78 68 55 66 68 53 77]]\n"
     ]
    }
   ],
   "source": [
    "calculate_metrics(preds_train, train_df['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = AudioDataset(test_data, audio_folder, is_train=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for mfccs, _ in test_loader:\n",
    "        outputs = model(mfccs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(test_data['id']).join(pd.DataFrame(data={'predictions':predictions}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_class_map = dict(zip(val_df['labels'], val_df['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['class'] = pred_df['predictions'].map(label_class_map)\n",
    "pred_df[['id', 'class']].to_csv('fourteenth_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model, 'models/fourteenth_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
